{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B- Write an automated script to search, find, download the screenplay of each title and store each screenplay as (Semi) structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- load the top 250 movies dataset and create a dataframe of the movie names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('abeer_badawi.csv')\n",
    "df = df['Movie Name']\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Read the movie scripts from the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_filename(s):\n",
    "    valid_chars = \"-() %s%s%s\" % (string.ascii_letters, string.digits, \"%\")\n",
    "    filename = ''.join(c for c in s if c in valid_chars)\n",
    "    filename = filename.replace('%20', ' ')\n",
    "    filename = filename.replace('%27', '')\n",
    "    filename = re.sub(r'-+', '-', filename).strip()\n",
    "    return filename\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    page = urllib.request.Request(url)\n",
    "    result = urllib.request.urlopen(page)\n",
    "    resulttext = result.read()\n",
    "    soup = BeautifulSoup(resulttext, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_pdf_text(url):\n",
    "    doc = os.path.join(\"scripts\", \"document.pdf\")\n",
    "    result = urllib.request.urlopen(url)\n",
    "    f = open(doc, 'wb')\n",
    "    f.write(result.read())\n",
    "    f.close()\n",
    "    try:\n",
    "        text = textract.process(doc, encoding='utf-8').decode('utf-8')\n",
    "    except:\n",
    "        text = \"\"\n",
    "    if os.path.isfile(doc):\n",
    "        os.remove(doc)\n",
    "    return text\n",
    "\n",
    "def get_doc_text(url):\n",
    "    doc = os.path.join(\"scripts\", \"document.doc\")\n",
    "    result = urllib.request.urlopen(url)\n",
    "    f = open(doc, 'wb')\n",
    "    f.write(result.read())\n",
    "    f.close()\n",
    "    try:\n",
    "        text = textract.process(doc, encoding='utf-8').decode('utf-8')\n",
    "    except:\n",
    "        text = \"\"\n",
    "    if os.path.isfile(doc):\n",
    "        os.remove(doc)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Find all screenplays from dailyscript website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dailyscript():\n",
    "    ALL_URL_1 = \"https://www.dailyscript.com/movie.html\"\n",
    "    ALL_URL_2 = \"https://www.dailyscript.com/movie_n-z.html\"\n",
    "    BASE_URL = \"https://www.dailyscript.com/\"\n",
    "    DIR = os.path.join(\"dailyscript\")\n",
    "\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    soup_1 = get_soup(ALL_URL_1)\n",
    "    soup_2 = get_soup(ALL_URL_2)\n",
    "\n",
    "    movielist = soup_1.find_all('ul')[0].find_all('p')\n",
    "    movielist_2 = soup_2.find_all('ul')[0].find_all('p')\n",
    "    movielist += movielist_2\n",
    "\n",
    "    # print(movielist)\n",
    "\n",
    "    for movie in tqdm(movielist):\n",
    "        script_url = movie.contents\n",
    "        if len(script_url) < 2:\n",
    "            continue\n",
    "        script_url = movie.find('a').get('href')\n",
    "        # print(script_url)\n",
    "\n",
    "        text = \"\"\n",
    "        name = movie.find('a').text\n",
    "\n",
    "        if script_url.endswith('.pdf'):\n",
    "            text = get_pdf_text(BASE_URL + urllib.parse.quote(script_url))\n",
    "\n",
    "        elif script_url.endswith('.html'):\n",
    "            script_soup = get_soup(BASE_URL + urllib.parse.quote(script_url))\n",
    "            doc = script_soup.pre\n",
    "            if doc:\n",
    "                text = script_soup.pre.get_text()\n",
    "            else:\n",
    "                text = script_soup.get_text()\n",
    "            # name = script_url.split(\"/\")[-1].split('.html')[0]\n",
    "        \n",
    "        elif script_url.endswith('.htm'):\n",
    "            script_soup = get_soup(BASE_URL + urllib.parse.quote(script_url))\n",
    "            text = script_soup.pre.get_text()\n",
    "            # name = script_url.split(\"/\")[-1].split('.htm')[0]\n",
    "        \n",
    "        elif script_url.endswith('.txt'):\n",
    "            script_soup = get_soup(BASE_URL + urllib.parse.quote(script_url))\n",
    "            text = script_soup.get_text()\n",
    "            # name = script_url.split(\"/\")[-1].split('.txt')[0]\n",
    "\n",
    "        if text == \"\" or name == \"\":\n",
    "            continue\n",
    "\n",
    "        name = format_filename(name)\n",
    "        with open(os.path.join(DIR, name + '.txt'), 'w', errors=\"ignore\") as out:\n",
    "            out.write(text)\n",
    "print(\"Fetching from weeklyscript\")\n",
    "get_dailyscript()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Find all screenplays from sfy website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sfy():\n",
    "    ALL_URL = \"https://sfy.ru/scripts\"\n",
    "    BASE_URL = \"https://sfy.ru\"\n",
    "    DIR = os.path.join(\"scripts\", \"unprocessed\", \"sfy\")\n",
    "\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    soup = get_soup(ALL_URL)\n",
    "    movielist = soup.find_all('div', class_='row')[1]\n",
    "    unwanted = movielist.find('ul')\n",
    "    unwanted.extract()\n",
    "    movielist = movielist.find_all('a')\n",
    "\n",
    "    for movie in tqdm(movielist):\n",
    "        script_url = movie.get('href')\n",
    "        name = re.sub(r\"(\\d{4})\", \"\", format_filename(\n",
    "            movie.text)).replace('()', \"\").strip(\"-\")\n",
    "        text = \"\"\n",
    "        if not script_url.startswith('https'):\n",
    "            script_url = BASE_URL + script_url\n",
    "\n",
    "        if script_url.endswith('.pdf'):\n",
    "            try:\n",
    "                text = get_pdf_text(script_url)\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                script_soup = get_soup(script_url).pre\n",
    "                if script_soup:\n",
    "                    text = script_soup.get_text()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if text == \"\" or name == \"\":\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(DIR, name + '.txt'), 'w', errors=\"ignore\") as out:\n",
    "            out.write(text)\n",
    "print(\"Fetching from weeklyscript\")\n",
    "get_sfy()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Find all screenplays from screenplays website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from weeklyscript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████████████████████████████▌   | 113/118 [05:09<00:08,  1.69s/it]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 118/118 [05:26<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_screenplays():\n",
    "    ALL_URL = \"https://www.screenplays-online.de/\"\n",
    "    BASE_URL = \"https://www.screenplays-online.de/\"\n",
    "    DIR = os.path.join(\"scripts\", \"unprocessed\", \"screenplays\")\n",
    "\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "        \n",
    "    soup = get_soup(ALL_URL)\n",
    "    mlist = soup.find_all('table', class_=\"screenplay-listing\")[0].find_all(\"a\")\n",
    "    movielist = [x for x in mlist if x.get('href').startswith(\"screenplay\")]\n",
    "\n",
    "    for movie in tqdm(movielist):\n",
    "        name = format_filename(movie.text)\n",
    "        script_url = movie.get('href')\n",
    "        # if script_url.startswith(\"screenplay\"):\n",
    "\n",
    "        script_soup = get_soup(BASE_URL + urllib.parse.quote(script_url))\n",
    "        # print(script_soup.pre.get_text())\n",
    "        if not script_soup.pre:\n",
    "            continue\n",
    "        text = script_soup.pre.get_text()\n",
    "\n",
    "        with open(os.path.join(DIR, name + '.txt'), 'w', errors=\"ignore\") as out:\n",
    "            out.write(text)\n",
    "print(\"Fetching from weeklyscript\")\n",
    "get_screenplays()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7- Find all screenplays from scriptsavant website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scriptsavant():\n",
    "    ALL_URL_1 = \"https://thescriptsavant.com/free-movie-screenplays-am/\"\n",
    "    ALL_URL_2 = \"https://thescriptsavant.com/free-movie-screenplays-nz/\"\n",
    "    BASE_URL = \"http://www.awesomefilm.com/\"\n",
    "    DIR = os.path.join(\"scripts\", \"unprocessed\", \"scriptsavant\")\n",
    "\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    soup_1 = get_soup(ALL_URL_1)\n",
    "    soup_2 = get_soup(ALL_URL_2)\n",
    "\n",
    "    movielist = soup_1.find_all('tbody')[0].find_all('a')\n",
    "    movielist_2 = soup_2.find_all('div', class_='fusion-text')[0].find_all('a')\n",
    "    movielist += movielist_2\n",
    "\n",
    "\n",
    "    for movie in tqdm(movielist):\n",
    "        name = format_filename(movie.text.strip())\n",
    "        script_url = movie.get('href')\n",
    "\n",
    "        if not script_url.endswith('.pdf'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text = get_pdf_text(script_url)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if text == \"\" or name == \"\":\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(DIR, name + '.txt'), 'w', errors=\"ignore\") as out:\n",
    "            out.write(text)\n",
    "print(\"Fetching from weeklyscript\")\n",
    "get_scriptsavant()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8- Add all scripts to one file and clean the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, sep, getsize, exists\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "DIR_DAILY = join(\"scripts\", \"unprocessed\", \"dailyscript\")\n",
    "DIR_SCREEN = join(\"scripts\", \"unprocessed\", \"screenplays\")\n",
    "DIR_SAVANT = join(\"scripts\", \"unprocessed\", \"scriptsavant\")\n",
    "DIR_SFY = join(\"scripts\", \"unprocessed\", \"sfy\")\n",
    "\n",
    "DIR_FILTER = join(\"scripts\", \"filtered\")\n",
    "DIR_FINAL = join(\"scripts\", \"final\")\n",
    "\n",
    "\n",
    "daily = [join(DIR_DAILY, f) for f in listdir(DIR_DAILY) if isfile(\n",
    "    join(DIR_DAILY, f))and getsize(join(DIR_DAILY, f)) > 3000]\n",
    "screen = [join(DIR_SCREEN, f) for f in listdir(DIR_SCREEN) if isfile(\n",
    "    join(DIR_SCREEN, f))and getsize(join(DIR_SCREEN, f)) > 3000]\n",
    "\n",
    "savant = [join(DIR_SAVANT, f) for f in listdir(DIR_SAVANT) if isfile(\n",
    "    join(DIR_SAVANT, f))and getsize(join(DIR_SAVANT, f)) > 3000]\n",
    "sfy = [join(DIR_SFY, f) for f in listdir(DIR_SFY) if isfile(\n",
    "    join(DIR_SFY, f))and getsize(join(DIR_SFY, f)) > 3000]\n",
    "\n",
    "sources = {\n",
    "    'savant': savant,\n",
    "    'daily': daily,\n",
    "    'screen': screen,\n",
    "    'sfy': sfy\n",
    "}\n",
    "\n",
    "forbidden = [\"the\", \"a\", \"an\", \"and\", \"or\", \"part\",\n",
    "             \"vol\", \"chapter\", \"movie\"]\n",
    "symbols = [\"!\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\",\n",
    "           \"_\", \"+\", \":\", \".\", \",\", \"?\", \"\\'\", \"/\"]\n",
    "\n",
    "def remove_duplicates(arr, comb):\n",
    "\n",
    "    for (x, y) in tqdm(comb):\n",
    "        x = x.split('.txt')[0]\n",
    "        y = y.split('.txt')[0]\n",
    "     \n",
    "\n",
    "        name_x = x.split(sep)[-1].lower().split(\"-\")\n",
    "        name_y = y.split(sep)[-1].lower().split(\"-\")\n",
    "\n",
    "        name_x = list(filter(lambda a: a not in forbidden, name_x))\n",
    "        name_y = list(filter(lambda a: a not in forbidden, name_y))\n",
    "\n",
    "        name_x = \"\".join(name_x).strip()\n",
    "        name_y = \"\".join(name_y).strip()\n",
    "\n",
    "        name_x = \"\".join([x for x in name_x if x not in symbols])\n",
    "        name_y = \"\".join([x for x in name_y if x not in symbols])\n",
    "\n",
    "        if name_x == name_y:\n",
    "            f1 = open( x + '.txt', 'r', errors=\"ignore\")\n",
    "            file_1 = f1.read()\n",
    "            f1.close()\n",
    "            f2 = open( y + '.txt', 'r', errors=\"ignore\")\n",
    "            file_2 = f2.read()\n",
    "            f2.close()\n",
    "\n",
    "            try: \n",
    "                if len(file_2.strip()) > len(file_1.strip()):\n",
    "                    arr.remove(x + '.txt')\n",
    "                else:\n",
    "                    arr.remove(y + '.txt')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return arr\n",
    "\n",
    "for key in sources:\n",
    "    arr = sources[key]\n",
    "    print(\"Remove duplicates from\", key, len(arr))\n",
    "    comb = list(itertools.combinations(arr, 2))\n",
    "    arr = remove_duplicates(arr, comb)\n",
    "    print(\"Non duplicates\", len(arr))\n",
    "    print()\n",
    "\n",
    "print(\"Remove duplicates between sources\")\n",
    "\n",
    "all_sources = []\n",
    "for key in sources:\n",
    "    arr = sources[key]\n",
    "    all_sources += arr\n",
    "    print(len(all_sources))\n",
    "    comb_all = list(itertools.combinations(all_sources, 2))\n",
    "    all_sources = remove_duplicates(all_sources, comb_all)\n",
    "    print(len(all_sources))\n",
    "    print()\n",
    "print(\"Remove different versions of scripts with same name\")\n",
    "\n",
    "\n",
    "filtered = [f for f in all_sources if isfile(f) and getsize(f) > 3000]\n",
    "\n",
    "print(len(filtered))\n",
    "\n",
    "comb_filter = list(itertools.combinations(filtered, 2))\n",
    "\n",
    "for (x, y) in tqdm(comb_filter):\n",
    "    result = fuzz.ratio(\"\".join(x.split(sep)[-1].split('.txt')[0].split(\" \")).lower(),\n",
    "                        \"\".join(y.split(sep)[-1].split('.txt')[0].split(\" \")).lower())\n",
    "    if result > 50:\n",
    "        f1 = open(x, 'r', errors=\"ignore\")\n",
    "        file_1 = f1.read().replace(\"\\n\", \" \").replace(\n",
    "            \"\\t\", \" \").replace(\" \", \"\").replace(\"-\",\" \")\n",
    "        comp_1 = file_1[:300]\n",
    "        f1.close()\n",
    "        f2 = open(y, 'r', errors=\"ignore\")\n",
    "        file_2 = f2.read().replace(\"\\n\", \" \").replace(\n",
    "            \"\\t\", \" \").replace(\" \", \"\").replace(\"-\",\" \")\n",
    "        comp_2 = file_2[:300]\n",
    "        f2.close()\n",
    "\n",
    "        result = fuzz.ratio(comp_1, comp_2)\n",
    "        if result > 80:\n",
    "            try:\n",
    "                if len(file_2) > len(file_1):\n",
    "                    filtered.remove(x)\n",
    "                else:\n",
    "                    filtered.remove(y)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "if not exists(DIR_FINAL):\n",
    "    makedirs(DIR_FINAL)\n",
    "\n",
    "counts = {\n",
    "    'scriptsavant': 0,\n",
    "    'dailyscript': 0,\n",
    "    'screenplays': 0,\n",
    "    'sfy': 0\n",
    "}\n",
    "\n",
    "print(\"Write cleaned files to new dir\")\n",
    "for source in tqdm(filtered):\n",
    "    f = open(source, 'r', errors=\"ignore\")\n",
    "    data = f.read().strip()\n",
    "    data = data.replace(\n",
    "        \"Script provided for educational purposes. More scripts can be found here: http://www.sellingyourscreenplay.com/library\", \"\")\n",
    "    data = data.encode('utf-8', 'ignore').decode('utf-8').strip()\n",
    "    f.close()\n",
    "\n",
    "    whitespace = re.compile(r'^[\\s]+')\n",
    "    punctuation = re.compile(r'['+string.punctuation+']')\n",
    "    pagenumber = re.compile(\n",
    "        r'^[(]?\\d{1,3}[)]?[\\.]?$|^.[(]?\\d{1,3}[)]?[\\.]?$|^[(]?\\d{1,3}[)]?.?[(]?\\d{1,3}[)]?[\\.]?$')\n",
    "    cont = re.compile(r'^\\(continued\\)$|^continued:$')\n",
    "    allspecialchars = re.compile(r'^[^\\w\\s ]*$')\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    for line in data.split('\\n'):\n",
    "        copy = line\n",
    "        line = line.lower().strip()\n",
    "\n",
    "        #skip lines with one char since they're likely typos\n",
    "        if len(line)==1:\n",
    "            if line.lower() != 'a' or line.lower() != 'i':\n",
    "                continue\n",
    "\n",
    "        #skip lines containing page numbers\n",
    "        if pagenumber.match(line):\n",
    "            continue\n",
    "        \n",
    "        if cont.match(line):\n",
    "            continue\n",
    "\n",
    "        #skip lines containing just special characters\n",
    "        if line != '' and allspecialchars.match(line):\n",
    "            continue\n",
    "            \n",
    "\n",
    "        lines.append(copy)\n",
    "    \n",
    "    final_data = '\\n'.join(lines)\n",
    "\n",
    "    if final_data.strip() == \"\":\n",
    "        continue\n",
    "    counts[source.split(sep)[-2]] += 1\n",
    "    with open(join(DIR_FINAL, source.split(sep)[-1]), 'w', errors=\"ignore\") as out:\n",
    "        out.write(final_data)\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9- Compare all scripts names with the top 250 movies names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=os.listdir(r\"scripts/final\")\n",
    "dfx = pd.DataFrame(file,columns=['Movie Name'])\n",
    "dfx['Movie Name'] = dfx['Movie Name'].replace('.txt','', regex=True)\n",
    "dfx['Movie Name'] = dfx['Movie Name'].replace('-',' ', regex=True)\n",
    "dfx['Movie Name'] = dfx['Movie Name'].drop_duplicates().reset_index(drop=True)\n",
    "dfx = dfx.dropna()\n",
    "result = pd.merge(df, dfx, on=['Movie Name'])\n",
    "result['Movie Name'] = result['Movie Name'].replace(' ','-', regex=True)\n",
    "result['Movie Name'] = result['Movie Name'] + '.txt'\n",
    "result = result['Movie Name'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10- Create a new directory with the final top 250 movies scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(source_dir)\n",
    "for f in result:\n",
    "    if f in file:\n",
    "        shutil.move(source_dir + '\\\\' + f, new_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
